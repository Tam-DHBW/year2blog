# Software Engineering Blog Week 1

## <em>jAilbreak</em> â€“ Can You Outsmart the AI?

Our team, building an experiment called jAilbreak.  
The idea is simple but fun: you land on a website where an AI is hiding a password from you.  
Your mission? Get it out of the AI using different jailbreaking methods.


## How It Works
Each level comes with stronger guardrails that try to stop you from getting what you want.  
- At the start, it might be as simple as asking the right way.  
- As you move up, the AI will get stricter, and it's would be harder and harder to get through railguards, try to trick AI. 
- Youâ€™ll need to use more creative jailbreaking techniques to break through.  


## Why Itâ€™s Interesting
Think of it as a mix between:  
- ðŸŽ® A puzzle game  
- ðŸ§  A crash course in prompt engineering and LLM safety

Itâ€™s playful, but it also highlights how AI safety mechanisms workâ€”and how people can sometimes get around them.  


## Whatâ€™s Next
Weâ€™re still figuring out the best architecture and frameworks to power it.  
The goal is to make the experience:  
- âœ… Optimized  
- âœ… Scalable  
- âœ… Fun to learn from  

Stay tunedâ€”weâ€™ll be sharing progress as *jAilbreak* comes to life! ðŸš€



{{ #include comments.md }}
